# ğŸ¤– RAG-Powered PDF Chatbot with Local LLM (Ollama + Streamlit UI)

Build a **Retrieval-Augmented Generation (RAG)** chatbot for local PDFs/texts using **ChromaDB**, **LangChain**, and **Ollama** with a **Mistral** modelâ€”completely offline, no OpenAI API needed.

---

## ğŸ“Œ Features

- Chat with your own PDFs or text data
- Powered by local LLMs via **Ollama**
- Uses **ChromaDB** for fast semantic search
- Lightweight, private, and fast
- Clean UI via **Streamlit**

---

## ğŸ—‚ï¸ Project Structure

```text
.
â”œâ”€â”€ chroma/ # Persisted ChromaDB vector store
â”œâ”€â”€ data/ # Source documents (PDFs/texts)
â”œâ”€â”€ get_embedding_function.py # Returns embedding function
â”œâ”€â”€ populate_database.py.py # Create vector embedding and store to the ChromaDB
â”œâ”€â”€ query_data.py # CLI version of the query flow
â”œâ”€â”€ app.py # Streamlit UI for the chatbot
â”œâ”€â”€ test_rag.py # Run few unit test cases
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file
```

---

## ğŸš€ Getting Started

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Pull Mistral via Ollama and start Ollama server

Make sure Ollama is instaled and then run:

```bash
ollama pull mistral
ollama serve
```

### 3. Create database

Create the Chroma DB.

```bash
python populate_database.py
```

---

## ğŸ§  Run the App

### 1.1: Streamlit UI (Recommended)

Launch the UI:

```bash
streamlit run app.py
```

- Input your question in the text field
- Get answers and see cited source chunks
- Uses local Chroma vector store + Mistral model

### 1.2: Command-Line Interface

```bash
python query_data.py "What are the rules for Monopoly?"
```

### 2. Run Test Cases

```bash
pytest -s
```

---

## ğŸ§± How It Works

1. Document Ingestion â€“ Load and chunk PDFs/text files
2. Embedding â€“ Embed text chunks using your preferred model (OllamaEmbeddings is used)
3. Vector Search â€“ Retrieve top 5 relevant chunks using Chroma
4. Prompting â€“ Feed context + user query into a prompt
5. LLM Answering â€“ Generate response using Ollama LLM
6. Citations â€“ Output source IDs used in the answer

---

## ğŸ–¼ï¸ Preview

![Landing Page](https://github.com/sarmishra/Python-RAG-Chatbot-With-Local-LLM/blob/main/RAG-Chatbot-WIth-Local-LLM-Preview.png)

---
